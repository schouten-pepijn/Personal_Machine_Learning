{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. trees: 20\n",
      "Max depth: 5\n",
      "Feature area: 0.367\n",
      "Feature bathrooms: 0.109\n",
      "Feature parking: 0.083\n",
      "Feature airconditioning_label: 0.076\n",
      "Feature stories: 0.065\n",
      "Feature bedrooms: 0.062\n",
      "Feature guestroom_label: 0.051\n",
      "Feature mainroad_label: 0.044\n",
      "Feature furnishingstatus_label: 0.044\n",
      "Feature prefarea_label: 0.043\n",
      "Feature basement_label: 0.033\n",
      "Feature hotwaterheating_label: 0.024\n",
      "+-------+--------------------+\n",
      "|  price|          prediction|\n",
      "+-------+--------------------+\n",
      "|7000000|   7257448.734636264|\n",
      "|7210000|    7458939.22333838|\n",
      "|7350000|   5298164.855151334|\n",
      "|7560000|   6526843.342434246|\n",
      "|8120000|1.1402739360943178E7|\n",
      "+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test RMSE: 1546198.65\n",
      "Test R2: 0.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/26 09:46:31 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import (\n",
    "    VectorAssembler, StringIndexer, OneHotEncoder)\n",
    "\n",
    "RANDOM_STATE = 87\n",
    "\n",
    "file_dir = \"/Users/pepijnschouten/Desktop/Python_Scripts/\" \\\n",
    "    \"Python_Scripts_Books/Distributed_ML_with_PySpark/\" \\\n",
    "        \"Python_Own_Files/Chapter 6/data/Housing.csv\"\n",
    "pandas_df = pd.read_csv(file_dir)\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"GBTRegression\")\n",
    "         .getOrCreate())\n",
    "\n",
    "pyspark_df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "\n",
    "cat_cols = [\"mainroad\", \"guestroom\", \"basement\",\n",
    "            \"hotwaterheating\", \"airconditioning\",\n",
    "            \"prefarea\", \"furnishingstatus\"]\n",
    "indexers = [StringIndexer(inputCol=col,\n",
    "                          outputCol=col+\"_label\",\n",
    "                          handleInvalid=\"keep\")\n",
    "            for col in cat_cols]\n",
    "\n",
    "for indexer in indexers:\n",
    "    pyspark_df = indexer.fit(pyspark_df).transform(pyspark_df)\n",
    "\n",
    "# encode categorical values\n",
    "encoder = OneHotEncoder(inputCols=[column+\"_label\"\n",
    "                                   for column in cat_cols],\n",
    "                        outputCols=[column+\"_encoded\"\n",
    "                                    for column in cat_cols])\n",
    "\n",
    "pyspark_df = encoder.fit(pyspark_df).transform(pyspark_df)\n",
    "\n",
    "feature_cols = [\"area\", \"bedrooms\", \"bathrooms\",\n",
    "                \"stories\", \"parking\"] + [col+\"_label\"\n",
    "                                         for col\n",
    "                                         in cat_cols]\n",
    "\n",
    "# vector assembler\n",
    "assembler = VectorAssembler(inputCols=feature_cols,\n",
    "                            outputCol=\"features\")\n",
    "\n",
    "pyspark_df = assembler.transform(pyspark_df)\n",
    "\n",
    "# train test split\n",
    "train_data, test_data = pyspark_df.randomSplit([0.8, 0.2],\n",
    "                                                seed=RANDOM_STATE)\n",
    "\n",
    "gbt_model = GBTRegressor(featuresCol=\"features\",\n",
    "                         labelCol=\"price\")\n",
    "spark_model = gbt_model.fit(train_data)\n",
    "\n",
    "predictions = spark_model.transform(test_data)\n",
    "\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"price\",\n",
    "                                     predictionCol=\"prediction\",\n",
    "                                     metricName=\"rmse\")\n",
    "rmse = evaluator_rmse.evaluate(predictions)\n",
    "\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"price\",\n",
    "                                   predictionCol=\"prediction\",\n",
    "                                   metricName=\"r2\")\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "\n",
    "# number of trees and depth\n",
    "num_trees = spark_model.getNumTrees\n",
    "max_depth = spark_model.getOrDefault(\"maxDepth\")\n",
    "print(f\"Num. trees: {num_trees}\")\n",
    "print(f\"Max depth: {max_depth}\")\n",
    "\n",
    "# feature importances\n",
    "feature_importances = spark_model.featureImportances\n",
    "feature_names = feature_cols\n",
    "feature_importances_tuples = list(zip(feature_names, feature_importances))\n",
    "sorted_feature_importance = sorted(feature_importances_tuples,\n",
    "                                  key=lambda x: x[1],\n",
    "                                  reverse=True)\n",
    "for feature, importance in sorted_feature_importance:\n",
    "    print(f\"Feature {feature}: {importance:.3f}\")\n",
    "\n",
    "# compare true and predicted values\n",
    "predictions.select(\"price\", \"prediction\").show(5)\n",
    "\n",
    "print(f\"Test RMSE: {rmse:.2f}\")\n",
    "print(f\"Test R2: {r2:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_scikit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
