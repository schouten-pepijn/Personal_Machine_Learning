{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"DiabetesLogReg\").getOrCreate())\n",
    "\n",
    "# read data\n",
    "def read_data(path):\n",
    "    pandas_df = pd.read_csv(path)\n",
    "    return spark.createDataFrame(pandas_df)\n",
    "\n",
    "# split data\n",
    "def split_data(spark_df):\n",
    "    return spark_df.randomSplit([0.8, 0.2],\n",
    "                                seed=87)\n",
    "    \n",
    "# vectorize features\n",
    "def vectorize_features(train_data, test_data, feature_cols):\n",
    "    assembler = VectorAssembler(inputCols=feature_cols,\n",
    "                                outputCol=\"features\")\n",
    "    return (assembler.transform(train_data),\n",
    "            assembler.transform(test_data))\n",
    "    \n",
    "# standardize features\n",
    "def scale_features(train_data, test_data):\n",
    "    scaler = StandardScaler(inputCol=\"features\",\n",
    "                            outputCol=\"scaled_features\")\n",
    "    scaled_model = scaler.fit(train_data)\n",
    "    return (scaled_model.transform(train_data),\n",
    "            scaled_model.transform(test_data))\n",
    "    \n",
    "# train model\n",
    "def train_model(train_data):\n",
    "    spark.conf.set(\"spark.seed\", \"87\")\n",
    "    log_reg = LogisticRegression(featuresCol=\"scaled_features\",\n",
    "                                 labelCol=\"Outcome\")\n",
    "    return log_reg.fit(train_data)\n",
    "\n",
    "# evaluate model\n",
    "def evaluate_model(model, test_data):\n",
    "    preds = model.transform(test_data)\n",
    "    preds = preds.withColumn(\"prediction\",\n",
    "                             col(\"prediction\").cast(\"int\"))\n",
    "    preds_and_labels = preds.select([\"prediction\", \"Outcome\"])\n",
    "    \n",
    "    tp = preds_and_labels[\n",
    "        (preds_and_labels.Outcome == 1) &\n",
    "        (preds_and_labels.prediction == 1)].count()\n",
    "    \n",
    "    tn = preds_and_labels[\n",
    "        (preds_and_labels.Outcome == 0) &\n",
    "        (preds_and_labels.prediction == 0)].count()\n",
    "    \n",
    "    fp = preds_and_labels[\n",
    "        (preds_and_labels.Outcome == 0) &\n",
    "        (preds_and_labels.prediction == 1)].count()\n",
    "    \n",
    "    fn = preds_and_labels[\n",
    "        (preds_and_labels.Outcome == 1) &\n",
    "        (preds_and_labels.prediction == 0)].count()\n",
    "    \n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    confusion = [[tp, fp], [fn, tn]]\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"Confusion Matrix:\\n{confusion}\")\n",
    "    preds.select('Outcome', 'prediction').show(5)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/14 10:06:12 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7721518987341772\n",
      "Precision: 0.6382978723404256\n",
      "Recall: 0.6122448979591837\n",
      "F1 Score: 0.625\n",
      "Confusion Matrix:\n",
      "[[30, 17], [19, 92]]\n",
      "+-------+----------+\n",
      "|Outcome|prediction|\n",
      "+-------+----------+\n",
      "|      1|         0|\n",
      "|      0|         0|\n",
      "|      0|         0|\n",
      "|      0|         0|\n",
      "|      0|         0|\n",
      "+-------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/pepijnschouten/Desktop/Python_Scripts/\" \\\n",
    "    \"Python_Scripts_Books/PySpark/Distributed_ML_with_\" \\\n",
    "        \"PySpark/Python_Own_Files/Chapter 7 Logistic Reg\" \\\n",
    "            \"/data/diabetes.csv\"\n",
    "            \n",
    "spark_df = read_data(path)\n",
    "\n",
    "# relevant columns\n",
    "columns = ['Pregnancies', 'Glucose',\n",
    "           'BloodPressure', 'BMI',\n",
    "           'DiabetesPedigreeFunction', 'Age',\n",
    "           'Outcome']\n",
    "\n",
    "spark_df = spark_df.select(columns)\n",
    "spark_df = spark_df.filter(\n",
    "    (col('Glucose') != 0) &\n",
    "    (col('BloodPressure') != 0) &\n",
    "    (col('BMI') != 0))\n",
    "\n",
    "train_data, test_data = split_data(spark_df)\n",
    "\n",
    "*feature_cols, _ = columns\n",
    "\n",
    "train_data, test_data = vectorize_features(\n",
    "    train_data, test_data, feature_cols)\n",
    "\n",
    "train_data, test_data = scale_features(\n",
    "    train_data, test_data)\n",
    "\n",
    "log_reg_model = train_model(train_data)\n",
    "\n",
    "evaluate_model(log_reg_model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_scikit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
